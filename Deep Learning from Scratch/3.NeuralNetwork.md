## 从感知机到神经网络
神经网络的一个重要性质是它可以自动地从数据中学习到合适的权重参数。\
用图来表示神经网络的话，如图所示。我们把最左边的一列称为输入层，最右边的一列称为输出层，中间的一列称为中间层。中间层有时也称为隐藏层。“隐藏”一词的意思是，隐藏层的神经元（和输入层、输出层不同）肉眼看不见。就神经元的连接方式而言，与感知机并没有任何差异。\
<img src="./image/12.png" width = "400" height = "300">
```
y =
  { 0 (b + w1x1 + w2x2 <= 0)
    1 (b + w1x1 + w2x2 >0) } (3.1)
y = h(b + w1x1 + w2x2) (3.2)
h(x) =
  { 0 (x <= 0)
    1 (x > 0) } (3.3)
```
式（3.2）中，输入信号的总和会被函数h(x)转换，转换后的值就是输出y。然后，式（3.3）所表示的函数h(x)，在输入超过0时返回1，否则返回0。因此，式（3.1）和式（3.2）、式（3.3）做的是相同的事情。\
这个感知机将x1、x2、1三个信号作为神经元的输入，将其和各自的权重相乘后，传送至下一个神经元。在下一个神经元中，计算这些加权信号的总和。如果这个总和超过0，则输出1，否则输出0。
## 激活函数
h（x）函数会将输入信号的总和转换为输出信号，这种函数一般称为激活函数（activation function）。如“激活”一词所示，激活函数的作用在于决定如何来激活输入信号的总和。 \
式（3.2）分两个阶段进行处理，先计算输入信号的加权总和，然后用激活函数转换这一总和。因此，如果将式（3.2）写得详细一点，则可以分成下面两个式子。
```
a = b + w1x1 + w2x2 （3.4）
y = h(a)
```
式（3.4）计算加权输入信号和偏置的总和，记为a。然后，式（3.5）用h()函数将a转换为输出y。之前的神经元都是用一个○表示的，如果要在图中明确表示出式（3.4）
和式（3.5），则可以像图3-4这样做。\
<img src="./image/13.png" width = "400" height = "300"> \
式（3.3）表示的激活函数以阈值为界，一旦输入超过阈值，就切换输出。这样的函数称为“阶跃函数”。因此，可以说感知机中使用了阶跃函数作为激活函数。也就是说，在激活函数的众多候选函数中，感知机使用了阶跃函数。
### sigmoid函数
```
h(x) = 1 / (1 + exp(-x))
exp(−x)表示e^−x
```
神经网络中用sigmoid函数作为激活函数，进行信号的转换，转换后的  信号被传送给下一个神经元。实际上，上一章介绍的感知机和接下来要介绍的神经网络的主要区别就在于这个激活函数。其他方面，比如神经元的多层连接的构造、信号的传递方法等，基本上和感知机是一样的。
### 阶跃函数的实现
```
def step_function(x):
  if x > 0:
    return 1
  else:
    return 0

def step_function(x): # 支持NumPy数组的实现
  y = x > 0
  return y.astype(np.int)
```
### 阶跃函数的图形
```
import numpy as np
import matplotlib.pylab as plt

def step_function(x):
  return np.array(x > 0, dtype=np.int)

x = np.arange(-5.0, 5.0, 0.1)
y = step_function(x)
plt.plot(x, y)
plt.ylim(-0.1, 1.1) # 指定y轴的范围
plt.show()
```
<img src="./image/14.png" width = "400" height = "300"> \
如图所示，阶跃函数以0为界，输出从0切换为1（或者从1切换为0）。它的值呈阶梯式变化，所以称为阶跃函数。
### sigmoid函数的实现
```
def sigmoid(x):
  return 1 / (1 + np.exp(-x))

x = np.arange(-5.0, 5.0, 0.1)
y = sigmoid(x)
plt.plot(x, y)
plt.ylim(-0.1, 1.1) # 指定y轴的范围
plt.show()
```
<img src="./image/15.png" width = "400" height = "300">\
### sigmoid函数和阶跃函数的比较
sigmoid函数是一条平滑的曲线，输出随着输入发生连续性的变化。而阶跃函数以0为界，输出发生急剧性的变化。sigmoid函数的平滑性对神经网络的学习具有重要意义。\
相对于阶跃函数只能返回0或1，sigmoid函数可以返回0.731 、0.880 等实数（这一点和刚才的平滑性有关）。\
也就是说，感知机中神经元之间流动的是0或1的二元信号，而神经网络中流动的是连续的实数值信号。 \
两者的结构均是“输入小时，输出接近0（为0）；随着输入增大，输出向1靠近（变成1）”。也就是说，当输入信号为重要信息时，阶跃函数和sigmoid函数都会输出较大的值；当输入信号为不重要的信息时，两者都输出较小的值。还有一个共同点是，不管输入信号有多小，或者有多大，输出信号的值都在0到1之间。
### 非线性函数
神经网络的激活函数必须使用非线性函数。换句话说，激活函数不能使用线性函数。为什么不能使用线性函数呢？因为使用线性函数的话，加深神经网络的层数就没有意义了。
### ReLU函数
ReLU函数在输入大于0时，直接输出该值；在输入小于等于0时，输出0。
```
h(x) = 
  { x (x > 0)
    0 (x <= 0) } (3.7)

def relu(x):
  return np.maximum(0, x)
```
<img src="./image/16.png" width = "400" height = "300"> 


